import os
import io
import logging
import base64
import img2pdf
import cv2
import numpy as np
from datetime import datetime
from PIL import Image, ImageOps, ImageEnhance
from google.cloud import vision
from pdf2image import convert_from_path
from services.storage import upload_file_to_cloud
from services.openai_client import analyze_document

logger = logging.getLogger(__name__)

if not os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "google_credentials.json"

class DocumentProcessor:
    def __init__(self):
        self.temp_dir = "temp_files"
        os.makedirs(self.temp_dir, exist_ok=True)
        self.vision_client = vision.ImageAnnotatorClient()

    def _fix_exif_orientation_pil(self, img):
        try: return ImageOps.exif_transpose(img)
        except: return img

    def _convert_pdf_to_jpg(self, pdf_path):
        try:
            return convert_from_path(pdf_path, dpi=200)
        except Exception as e:
            logger.error(f"PDF->JPG error: {e}")
            return None

    def _google_vision_process(self, pil_image, is_retry=False):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (processed_image, extracted_text)
        is_retry: –§–ª–∞–≥, —á—Ç–æ —ç—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –ø—Ä–æ–≥–æ–Ω –ø–æ—Å–ª–µ –ø–æ–≤–æ—Ä–æ—Ç–∞
        """
        extracted_text = ""
        try:
            img_byte_arr = io.BytesIO()
            pil_image.save(img_byte_arr, format='JPEG')
            content = img_byte_arr.getvalue()
            image = vision.Image(content=content)
            
            # 1. –ó–ê–ü–†–û–° –ö GOOGLE
            response = self.vision_client.document_text_detection(image=image)
            
            if response.error.message:
                logger.error(f"Google Error: {response.error.message}")
                return pil_image, ""

            # –¢–µ–∫—Å—Ç –¥–ª—è OpenAI
            if response.full_text_annotation:
                extracted_text = response.full_text_annotation.text

            # 2. –õ–æ–≥–∏–∫–∞ –ü–û–í–û–†–û–¢–ê (–¢–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥)
            if not is_retry and response.full_text_annotation.pages:
                page = response.full_text_annotation.pages[0]
                if page.blocks:
                    # –°—á–∏—Ç–∞–µ–º —É–≥–æ–ª –ø–æ –ø–µ—Ä–≤–æ–º—É —Å–ª–æ–≤—É
                    word = page.blocks[0].paragraphs[0].words[0]
                    v = word.bounding_box.vertices
                    dx = v[1].x - v[0].x
                    dy = v[1].y - v[0].y
                    import math
                    rotation_angle = math.degrees(math.atan2(dy, dx))
                    
                    final_rotation = 0
                    if 45 <= rotation_angle < 135: final_rotation = 90
                    elif -135 < rotation_angle <= -45: final_rotation = -90
                    elif rotation_angle >= 135 or rotation_angle <= -135: final_rotation = 180
                    
                    if final_rotation != 0:
                        logger.info(f"üîÑ Rotation needed: {final_rotation} (Detected: {rotation_angle:.2f})")
                        if final_rotation == 90: pil_image = pil_image.rotate(90, expand=True)
                        elif final_rotation == -90: pil_image = pil_image.rotate(-90, expand=True)
                        elif final_rotation == 180: pil_image = pil_image.rotate(180, expand=True)
                        
                        # –†–ï–ö–£–†–°–ò–Ø: –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∑–∞–Ω–æ–≤–æ –¥–ª—è —É–∂–µ –ø–æ–≤–µ—Ä–Ω—É—Ç–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏, 
                        # —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –∫—Ä–æ–ø–∞.
                        return self._google_vision_process(pil_image, is_retry=True)

            # 3. –õ–æ–≥–∏–∫–∞ –û–ë–†–ï–ó–ö–ò (Smart Cluster Crop)
            if response.full_text_annotation:
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –±–ª–æ–∫–∏
                blocks = []
                for page in response.full_text_annotation.pages:
                    for block in page.blocks:
                        v = block.bounding_box.vertices
                        min_x = min(p.x for p in v)
                        min_y = min(p.y for p in v)
                        max_x = max(p.x for p in v)
                        max_y = max(p.y for p in v)
                        area = (max_x - min_x) * (max_y - min_y)
                        blocks.append({'box': (min_x, min_y, max_x, max_y), 'area': area})

                if not blocks:
                    return pil_image, extracted_text

                # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –±–ª–æ–∫ (—Ç–µ–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞)
                blocks.sort(key=lambda x: x['area'], reverse=True)
                main_block = blocks[0]
                mb = main_block['box'] # (x1, y1, x2, y2)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã, –Ω–∞—á–∏–Ω–∞—è —Å –≥–ª–∞–≤–Ω–æ–≥–æ –±–ª–æ–∫–∞
                final_min_x, final_min_y, final_max_x, final_max_y = mb

                # –î–æ–±–∞–≤–ª—è–µ–º –¥—Ä—É–≥–∏–µ –±–ª–æ–∫–∏, –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –æ–Ω–∏ –±–ª–∏–∑–∫–æ –∫ –≥–ª–∞–≤–Ω–æ–º—É (–∑–∞—â–∏—Ç–∞ –æ—Ç –∫—Ä–µ–¥–∏—Ç–æ–∫ –≤–Ω–∏–∑—É)
                doc_height = mb[3] - mb[1]
                threshold_y = doc_height * 0.3 # –î–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑—Ä—ã–≤ –Ω–µ –±–æ–ª–µ–µ 30% –æ—Ç –≤—ã—Å–æ—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞

                for b in blocks[1:]:
                    bx = b['box']
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –≥–ª–∞–≤–Ω–æ–≥–æ –±–ª–æ–∫–∞
                    dist_to_bottom = bx[1] - final_max_y # –ï—Å–ª–∏ –±–ª–æ–∫ –Ω–∏–∂–µ
                    dist_to_top = final_min_y - bx[3]    # –ï—Å–ª–∏ –±–ª–æ–∫ –≤—ã—à–µ
                    
                    # –ï—Å–ª–∏ –±–ª–æ–∫ —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ - —ç—Ç–æ –º—É—Å–æ—Ä (–∫—Ä–µ–¥–∏—Ç–∫–∞, –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞), –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
                    if dist_to_bottom > threshold_y or dist_to_top > threshold_y:
                        continue
                    
                    # –ò–Ω–∞—á–µ —Ä–∞—Å—à–∏—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã
                    final_min_x = min(final_min_x, bx[0])
                    final_min_y = min(final_min_y, bx[1])
                    final_max_x = max(final_max_x, bx[2])
                    final_max_y = max(final_max_y, bx[3])

                # –ü–∞–¥–¥–∏–Ω–≥
                pad = 20
                w_orig, h_orig = pil_image.size
                final_min_x = max(0, final_min_x - pad)
                final_min_y = max(0, final_min_y - pad)
                final_max_x = min(w_orig, final_max_x + pad)
                final_max_y = min(h_orig, final_max_y + pad)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
                area_crop = (final_max_x - final_min_x) * (final_max_y - final_min_y)
                ratio = area_crop / (w_orig * h_orig)

                if ratio < 0.15:
                    logger.warning(f"‚ö†Ô∏è Area too small ({ratio:.1%}). Skipping crop.")
                    return pil_image, extracted_text
                
                logger.info(f"‚úÇÔ∏è Smart Crop: {final_min_x},{final_min_y} -> {final_max_x},{final_max_y}")
                pil_image = pil_image.crop((final_min_x, final_min_y, final_max_x, final_max_y))

            return pil_image, extracted_text

        except Exception as e:
            logger.error(f"Google Vision Error: {e}")
            return pil_image, extracted_text

    def _enhance_image(self, pil_image):
        enhancer = ImageEnhance.Contrast(pil_image)
        pil_image = enhancer.enhance(1.2)
        return pil_image

    def _encode_image(self, path):
        with open(path, "rb") as f: return base64.b64encode(f.read()).decode('utf-8')

    def process_and_upload(self, user_phone, local_path, original_filename):
        is_pdf = local_path.lower().endswith(".pdf")
        processed_results = []
        pil_images = []
        
        try:
            if is_pdf: pil_images = self._convert_pdf_to_jpg(local_path)
            else: pil_images = [self._fix_exif_orientation_pil(Image.open(local_path))]
        except Exception as e: return [{"status": "error", "message": f"Read error: {e}"}]

        if not pil_images: return [{"status": "error", "message": "No images"}]
        source_file_uploaded = False

        for i, img in enumerate(pil_images, start=1):
            page_suffix = f"_page{i}"
            temp_page_jpg = os.path.join(self.temp_dir, f"temp_{user_phone}_p{i}.jpg")
            
            try:
                # 1. Google Vision
                img, ocr_text = self._google_vision_process(img)

                # 2. Enhance & Save
                img = self._enhance_image(img)
                img.save(temp_page_jpg, "JPEG", quality=90)
                
                # 3. Classify (OpenAI) - –ì–ò–ë–†–ò–î–ù–´–ô –ú–ï–¢–û–î
                doc_data = {"doc_type": "Document", "person_name": "Unknown"}
                
                # –ï—Å–ª–∏ OCR —Ç–µ–∫—Å—Ç–∞ –º–Ω–æ–≥–æ (>50 —Å–∏–º–≤–æ–ª–æ–≤) -> —à–ª–µ–º —Ç–µ–∫—Å—Ç (–±—ã—Å—Ç—Ä–æ, –±–µ–∑ —Ü–µ–Ω–∑—É—Ä—ã)
                # –ï—Å–ª–∏ –º–∞–ª–æ (—à—Ç–∞–º–ø—ã) -> —à–ª–µ–º –ö–ê–†–¢–ò–ù–ö–£ (–Ω–∞–¥–µ–∂–Ω–µ–µ –¥–ª—è —à—Ç–∞–º–ø–æ–≤)
                
                prompt = ""
                image_arg = None
                            
                if ocr_text and len(ocr_text) > 50:
                    logger.info("üöÄ Sending OCR Text to OpenAI")
                    # –ò–ó–ú–ï–ù–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢: –ë–æ–ª–µ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π, —á—Ç–æ–±—ã –æ–±–æ–π—Ç–∏ Safety Filter
                    prompt = f"""
                    Act as a Data Extraction API. Your task is to extract structured data from OCR text for an internal filing system.
                    
                    OCR TEXT:
                    '''{ocr_text[:3000]}''' 
                    
                    INSTRUCTIONS:
                    1. Identify the Document Type (e.g., Israeli_ID, Passport, Marriage_Certificate).
                    2. Extract the Full Name of the document holder. 
                       - Convert Hebrew names to Latin (English) characters.
                       - Example: "◊ô◊©◊®◊ê◊ú ◊ô◊©◊®◊ê◊ú◊ô" -> "Israel_Israeli".
                       - Ignore labels like "Name", "Surname". Just return the value.
                    
                    OUTPUT JSON ONLY:
                    {{"doc_type": "...", "person_name": "..."}}
                    """
                    image_arg = None    
                else:
                    logger.warning("‚ö†Ô∏è Little text found, sending IMAGE to OpenAI")
                    image_arg = self._encode_image(temp_page_jpg)
                    prompt = """
                    Classify document and extract Name (Latin).
                    JSON: {{"doc_type": "...", "person_name": "..."}}
                    """

                try:
                    res = analyze_document(image_arg, prompt)
                    if res: doc_data = res
                except Exception as e: logger.error(f"AI Classify Error: {e}")

                # 4. Save PDF & Upload
                final_pdf_path = os.path.join(self.temp_dir, f"temp_{user_phone}_p{i}.pdf")
                with open(temp_page_jpg, "rb") as f: pdf_bytes = img2pdf.convert(f.read())
                with open(final_pdf_path, "wb") as f: f.write(pdf_bytes)

                person = "".join(c for c in doc_data.get('person_name', 'Client') if c.isalnum() or c in ' _-').strip()
                base_folder = f"/Clients/{user_phone}/{person or 'Client'}"
                date_s = datetime.now().strftime("%Y-%m-%d")
                dtype = doc_data.get('doc_type', 'Doc')
                remote_filename = f"{date_s}_{dtype}{page_suffix}.pdf"
                remote_path_pdf = f"{base_folder}/{remote_filename}"

                if not source_file_uploaded:
                    orig_ext = os.path.splitext(local_path)[1] or ".jpg"
                    remote_orig = f"{base_folder}/Originals/{date_s}_{dtype}_Source_orig{orig_ext}"
                    try:
                        upload_file_to_cloud(local_path, remote_orig)
                        source_file_uploaded = True
                    except: pass

                if upload_file_to_cloud(final_pdf_path, remote_path_pdf):
                    processed_results.append({
                        "status": "success", "doc_type": dtype, "person": person, 
                        "filename": remote_filename, "remote_path": remote_path_pdf
                    })
                else:
                    processed_results.append({"status": "error", "message": "Upload failed"})

            except Exception as e:
                logger.error(f"Process Page {i} Error: {e}")
                processed_results.append({"status": "error", "message": str(e)})
            finally:
                for p in {temp_page_jpg, final_pdf_path}:
                    if p and os.path.exists(p): os.remove(p)

        return processed_results